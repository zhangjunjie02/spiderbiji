
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>2. Scrapy框架的使用 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="3.html" />
    
    
    <link rel="prev" href="1.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../">
            
                <a href="../">
            
                    
                    目录
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="./">
            
                <a href="./">
            
                    
                    一、Python网络爬虫进阶实战(上)
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="1.html">
            
                <a href="1.html">
            
                    
                    1. Scrapy框架介绍与安装
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.2" data-path="2.html">
            
                <a href="2.html">
            
                    
                    2. Scrapy框架的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="3.html">
            
                <a href="3.html">
            
                    
                    3. Selector选择器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="4.html">
            
                <a href="4.html">
            
                    
                    4. Spider的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="5.html">
            
                <a href="5.html">
            
                    
                    5. Downloader Middleware的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="6.html">
            
                <a href="6.html">
            
                    
                    6. Spider Middleware的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.7" data-path="7.html">
            
                <a href="7.html">
            
                    
                    7. ItemPipeline的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.8" data-path="8.html">
            
                <a href="8.html">
            
                    
                    8. Scrapy爬虫案例实战
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.9" data-path="9.html">
            
                <a href="9.html">
            
                    
                    本周作业
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../week02/">
            
                <a href="../week02/">
            
                    
                    二、Python网络爬虫进阶实战(中)
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../week02/1.html">
            
                <a href="../week02/1.html">
            
                    
                    09. Selenium的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../week02/2.html">
            
                <a href="../week02/2.html">
            
                    
                    10. Selenium爬取淘宝商品
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../week02/3.html">
            
                <a href="../week02/3.html">
            
                    
                    11. MongoDB数据库
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../week02/4.html">
            
                <a href="../week02/4.html">
            
                    
                    12. Scrapy框架使用Selenium
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../week02/5.html">
            
                <a href="../week02/5.html">
            
                    
                    13. 代理的使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="../week02/6.html">
            
                <a href="../week02/6.html">
            
                    
                    14. 使用代理爬取信息实战
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="../week02/7.html">
            
                <a href="../week02/7.html">
            
                    
                    15. Redis数据库
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="../week02/8.html">
            
                <a href="../week02/8.html">
            
                    
                    16. 分布式爬虫原理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="../week02/9.html">
            
                <a href="../week02/9.html">
            
                    
                    17. Scrapy分布式实战
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="../week02/10.html">
            
                <a href="../week02/10.html">
            
                    
                    本周作业
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >2. Scrapy框架的使用</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="2-scrapy&#x6846;&#x67B6;&#x7684;&#x4F7F;&#x7528;">2. Scrapy&#x6846;&#x67B6;&#x7684;&#x4F7F;&#x7528;</h1>
<h3 id="21-scrapy&#x6846;&#x67B6;&#x7684;&#x547D;&#x4EE4;&#x4ECB;&#x7ECD;">2.1 Scrapy&#x6846;&#x67B6;&#x7684;&#x547D;&#x4EE4;&#x4ECB;&#x7ECD;</h3>
<h4 id="scrapy-&#x547D;&#x4EE4;-&#x5206;&#x4E3A;&#x4E24;&#x79CD;&#xFF1A;&#x5168;&#x5C40;&#x547D;&#x4EE4;-&#x548C;-&#x9879;&#x76EE;&#x547D;&#x4EE4;&#x3002;">Scrapy &#x547D;&#x4EE4; &#x5206;&#x4E3A;&#x4E24;&#x79CD;&#xFF1A;<code>&#x5168;&#x5C40;&#x547D;&#x4EE4;</code> &#x548C; <code>&#x9879;&#x76EE;&#x547D;&#x4EE4;</code>&#x3002;</h4>
<ul>
<li><p><code>&#x5168;&#x5C40;&#x547D;&#x4EE4;</code>&#xFF1A;&#x5728;&#x54EA;&#x91CC;&#x90FD;&#x80FD;&#x4F7F;&#x7528;&#x3002;</p>
</li>
<li><p><code>&#x9879;&#x76EE;&#x547D;&#x4EE4;</code>&#xFF1A;&#x5FC5;&#x987B;&#x5728;&#x722C;&#x866B;&#x9879;&#x76EE;&#x91CC;&#x9762;&#x624D;&#x80FD;&#x4F7F;&#x7528;&#x3002;</p>
</li>
</ul>
<h4 id="&#x5168;&#x5C40;&#x547D;&#x4EE4;">&#x5168;&#x5C40;&#x547D;&#x4EE4;</h4>
<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

&#x4F7F;&#x7528;&#x683C;&#x5F0F;:
  scrapy &lt;command&gt; [options] [args]

&#x53EF;&#x7528;&#x7684;&#x547D;&#x4EE4;:
  bench         &#x6D4B;&#x8BD5;&#x672C;&#x5730;&#x786C;&#x4EF6;&#x6027;&#x80FD;&#xFF08;&#x5DE5;&#x4F5C;&#x539F;&#x7406;&#xFF1A;&#xFF09;&#xFF1A;scrapy bench
  commands
  fetch         &#x53D6;URL&#x4F7F;&#x7528;Scrapy&#x4E0B;&#x8F7D;
  genspider     &#x4EA7;&#x751F;&#x65B0;&#x7684;&#x8718;&#x86DB;&#x4F7F;&#x7528;&#x9884;&#x5148;&#x5B9A;&#x4E49;&#x7684;&#x6A21;&#x677F;
  runspider     &#x8FD0;&#x7528;&#x5355;&#x72EC;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF1A;scrapy runspider abc.py
  settings      &#x83B7;&#x53D6;&#x8BBE;&#x7F6E;&#x503C;
  shell         &#x8FDB;&#x5165;&#x4EA4;&#x4E92;&#x7EC8;&#x7AEF;&#xFF0C;&#x7528;&#x4E8E;&#x722C;&#x866B;&#x7684;&#x8C03;&#x8BD5;&#xFF08;&#x5982;&#x679C;&#x4F60;&#x4E0D;&#x8C03;&#x8BD5;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x4E0D;&#x5E38;&#x7528;&#xFF09;&#xFF1A;scrapy shell http://www.baidu.com --nolog&#xFF08;--nolog &#x4E0D;&#x663E;&#x793A;&#x65E5;&#x5FD7;&#x4FE1;&#x606F;&#xFF09;
  startproject  &#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x9879;&#x76EE;&#xFF0C;&#x5982;&#xFF1A;scrapy startproject demo&#xFF08;demo &#x521B;&#x5EFA;&#x7684;&#x722C;&#x866B;&#x9879;&#x76EE;&#x7684;&#x540D;&#x5B57;&#xFF09;
  version       &#x67E5;&#x770B;&#x7248;&#x672C;&#xFF1A;&#xFF08;scrapy version&#xFF09;
  view          &#x4E0B;&#x8F7D;&#x4E00;&#x4E2A;&#x7F51;&#x9875;&#x7684;&#x6E90;&#x4EE3;&#x7801;&#xFF0C;&#x5E76;&#x5728;&#x9ED8;&#x8BA4;&#x7684;&#x6587;&#x672C;&#x7F16;&#x8F91;&#x5668;&#x4E2D;&#x6253;&#x5F00;&#x8FD9;&#x4E2A;&#x6E90;&#x4EE3;&#x7801;&#xFF1A;scrapy view http://www.aobossir.com/

  [ more ]      &#x4ECE;&#x9879;&#x76EE;&#x76EE;&#x5F55;&#x8FD0;&#x884C;&#x65F6;&#x53EF;&#x83B7;&#x5F97;&#x66F4;&#x591A;&#x547D;&#x4EE4;

&#x4F7F;&#x7528; &quot;scrapy &lt;command&gt; -h&quot; &#x8981;&#x67E5;&#x770B;&#x6709;&#x5173;&#x547D;&#x4EE4;&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;
</code></pre><h4 id="&#x9879;&#x76EE;&#x547D;&#x4EE4;&#xFF1A;">&#x9879;&#x76EE;&#x547D;&#x4EE4;&#xFF1A;</h4>
<pre><code>D:\BaiduYunDownload\first&gt;scrapy -h
Scrapy 1.2.1 - project: first

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  commands
  crawl         &#x8FD0;&#x884C;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x6587;&#x4EF6;&#x3002;&#xFF1A;scrapy crawl f1 &#x6216;&#x8005; scrapy crawl f1 --nolog
  edit          &#x4F7F;&#x7528;&#x7F16;&#x8F91;&#x5668;&#x6253;&#x5F00;&#x722C;&#x866B;&#x6587;&#x4EF6; &#xFF08;Windows&#x4E0A;&#x4F3C;&#x4E4E;&#x6709;&#x95EE;&#x9898;&#xFF0C;Linux&#x4E0A;&#x6CA1;&#x6709;&#x95EE;&#x9898;&#xFF09;&#xFF1A;scrapy edit f1
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          &#x5217;&#x51FA;&#x5F53;&#x524D;&#x722C;&#x866B;&#x9879;&#x76EE;&#x4E0B;&#x6240;&#x6709;&#x7684;&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF1A; scrapy list
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      &#x83B7;&#x53D6;&#x8BBE;&#x7F6E;&#x503C;
  shell         &#x8FDB;&#x5165;&#x4EA4;&#x4E92;&#x7EC8;&#x7AEF;&#xFF0C;&#x7528;&#x4E8E;&#x722C;&#x866B;&#x7684;&#x8C03;&#x8BD5;&#xFF08;&#x5982;&#x679C;&#x4F60;&#x4E0D;&#x8C03;&#x8BD5;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x4E0D;&#x5E38;&#x7528;&#xFF09;
  startproject  &#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x9879;&#x76EE;&#xFF0C;&#x5982;&#xFF1A;scrapy startproject demo&#xFF08;demo &#x521B;&#x5EFA;&#x7684;&#x722C;&#x866B;&#x9879;&#x76EE;&#x7684;&#x540D;&#x5B57;&#xFF09;
  version       &#x67E5;&#x770B;&#x7248;&#x672C;&#xFF1A;&#xFF08;scrapy version&#xFF09;
  view          &#x4E0B;&#x8F7D;&#x4E00;&#x4E2A;&#x7F51;&#x9875;&#x7684;&#x6E90;&#x4EE3;&#x7801;&#xFF0C;&#x5E76;&#x5728;&#x9ED8;&#x8BA4;&#x7684;&#x6587;&#x672C;&#x7F16;&#x8F91;&#x5668;&#x4E2D;&#x6253;&#x5F00;&#x8FD9;&#x4E2A;&#x6E90;&#x4EE3;&#x7801;

Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command
</code></pre><p>&#x6CE8;&#x610F;&#xFF1A;Scrapy&#x8FD0;&#x884C;ImportError: No module named win32api&#x9519;&#x8BEF;&#x3002;&#x8BF7;&#x5B89;&#x88C5;&#xFF1A;pip install pypiwin32</p>
<h4 id="&#x3000;scrapy&#x6846;&#x67B6;&#x7684;&#x547D;&#x4EE4;&#x4F7F;&#x7528;&#xFF1A;">&#x3000;Scrapy&#x6846;&#x67B6;&#x7684;&#x547D;&#x4EE4;&#x4F7F;&#x7528;&#xFF1A;</h4>
<ul>
<li><p>&#x67E5;&#x770B;&#x6240;&#x6709;&#x547D;&#x4EE4;</p>
<pre><code>scrapy -h
</code></pre></li>
<li><p>&#x67E5;&#x770B;&#x5E2E;&#x52A9;&#x4FE1;&#x606F;:</p>
<pre><code>scapy --help
</code></pre></li>
<li><p>&#x67E5;&#x770B;&#x7248;&#x672C;&#x4FE1;&#x606F;:</p>
<pre><code>(venv)ql@ql:~$ scrapy version
Scrapy 1.1.2
(venv)ql@ql:~$ 
(venv)ql@ql:~$ scrapy version -v
Scrapy    : 1.1.2
lxml      : 3.6.4.0
libxml2   : 2.9.4
Twisted   : 16.4.0
Python    : 2.7.12 (default, Jul  1 2016, 15:12:24) - [GCC 5.4.0 20160609]
pyOpenSSL : 16.1.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)
Platform  : Linux-4.4.0-36-generic-x86_64-with-Ubuntu-16.04-xenial
(venv)ql@ql:~$
</code></pre></li>
<li><p>&#x65B0;&#x5EFA;&#x4E00;&#x4E2A;&#x5DE5;&#x7A0B;</p>
</li>
</ul>
<pre><code>scrapy startproject spider_name
</code></pre><ul>
<li>&#x6784;&#x5EFA;&#x722C;&#x866B;genspider(generator spider)</li>
<li><p>&#x4E00;&#x4E2A;&#x5DE5;&#x7A0B;&#x4E2D;&#x53EF;&#x4EE5;&#x5B58;&#x5728;&#x591A;&#x4E2A;spider, &#x4F46;&#x662F;&#x540D;&#x5B57;&#x5FC5;&#x987B;&#x552F;&#x4E00;</p>
<pre><code>scrapy genspider name domain
#&#x5982;:
#scrapy genspider sohu sohu.org
</code></pre></li>
<li><p>&#x67E5;&#x770B;&#x5F53;&#x524D;&#x9879;&#x76EE;&#x5185;&#x6709;&#x591A;&#x5C11;&#x722C;&#x866B;</p>
<pre><code>scrapy list
</code></pre></li>
<li><p>view&#x4F7F;&#x7528;&#x6D4F;&#x89C8;&#x5668;&#x6253;&#x5F00;&#x7F51;&#x9875;</p>
<pre><code>scrapy view http://www.baidu.com
</code></pre></li>
</ul>
<h4 id="shell&#x547D;&#x4EE4;-&#x8FDB;&#x5165;scrpay&#x4EA4;&#x4E92;&#x73AF;&#x5883;">shell&#x547D;&#x4EE4;, &#x8FDB;&#x5165;scrpay&#x4EA4;&#x4E92;&#x73AF;&#x5883;</h4>
<pre><code># &#x8FDB;&#x5165;&#x8BE5;url&#x7684;&#x4EA4;&#x4E92;&#x73AF;&#x5883;
scrapy shell http://www.dmoz.org/Computers/Programming/Languages/Python/Books/
</code></pre><ul>
<li><p>&#x4E4B;&#x540E;&#x4FBF;&#x8FDB;&#x5165;&#x4EA4;&#x4E92;&#x73AF;&#x5883;&#xFF0C;&#x6211;&#x4EEC;&#x4E3B;&#x8981;&#x4F7F;&#x7528;&#x8FD9;&#x91CC;&#x9762;&#x7684;response&#x547D;&#x4EE4;, &#x4F8B;&#x5982;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;</p>
<pre><code>response.xpath()    #&#x62EC;&#x53F7;&#x91CC;&#x76F4;&#x63A5;&#x52A0;xpath&#x8DEF;&#x5F84;
</code></pre></li>
<li><p>runspider&#x547D;&#x4EE4;&#x7528;&#x4E8E;&#x76F4;&#x63A5;&#x8FD0;&#x884C;&#x521B;&#x5EFA;&#x7684;&#x722C;&#x866B;, &#x5E76;&#x4E0D;&#x4F1A;&#x8FD0;&#x884C;&#x6574;&#x4E2A;&#x9879;&#x76EE;</p>
<pre><code>scrapy runspider &#x722C;&#x866B;&#x540D;&#x79F0;
</code></pre></li>
</ul>
<h2 id="22-scrapy&#x6846;&#x67B6;&#x7684;&#x4F7F;&#x7528;&#xFF1A;">2.2 Scrapy&#x6846;&#x67B6;&#x7684;&#x4F7F;&#x7528;&#xFF1A;</h2>
<ul>
<li><p>&#x63A5;&#x4E0B;&#x6765;&#x901A;&#x8FC7;&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x9879;&#x76EE;&#xFF0C;&#x5B8C;&#x6210;&#x4E00;&#x904D;Scrapy&#x6293;&#x53D6;&#x6D41;&#x7A0B;&#x3002;</p>
</li>
<li><p>&#x5177;&#x4F53;&#x6D41;&#x7A0B;&#x5982;&#x4E0B;&#xFF1A;</p>
<ul>
<li>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;scrapy&#x9879;&#x76EE;&#xFF1A;</li>
<li>&#x521B;&#x952E;&#x4E00;&#x4E2A;Spider&#x6765;&#x6293;&#x53D6;&#x7AD9;&#x70B9;&#x548C;&#x5904;&#x7406;&#x6570;&#x636E;&#x3002;</li>
<li>&#x5230;&#x8FC7;&#x547D;&#x4EE4;&#x884C;&#x5C06;&#x6293;&#x53D6;&#x7684;&#x6293;&#x53D6;&#x5185;&#x5BB9;&#x5BFC;&#x51FA;</li>
</ul>
</li>
</ul>
<h4 id="&#x2460;-&#x521B;&#x5EFA;&#x9879;&#x76EE;">&#x2460; &#x521B;&#x5EFA;&#x9879;&#x76EE;</h4>
<ul>
<li><p>&#x722C;&#x53D6;&#x6211;&#x7231;&#x6211;&#x5BB6;&#x7684;&#x697C;&#x76D8;&#x4FE1;&#x606F;&#xFF1A;</p>
</li>
<li><p>&#x7F51;&#x5740;&#xFF1A; <a href="https://fang.5i5j.com/bj/loupan/" target="_blank">https://fang.5i5j.com/bj/loupan/</a></p>
</li>
<li><p>&#x5728;&#x547D;&#x4EE4;&#x884C;&#x7F16;&#x5199;&#x4E0B;&#x9762;&#x547D;&#x4EE4;&#xFF0C;&#x521B;&#x5EFA;&#x9879;&#x76EE;demo </p>
</li>
</ul>
<pre><code>scrapy startproject  demo
</code></pre><ul>
<li>&#x9879;&#x76EE;&#x76EE;&#x5F55;&#x7ED3;&#x6784;&#xFF1A;</li>
</ul>
<pre><code>demo
&#x251C;&#x2500;&#x2500; demo
&#x2502;   &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;   &#x251C;&#x2500;&#x2500; __pycache__
&#x2502;   &#x251C;&#x2500;&#x2500; items.py        # Items&#x7684;&#x5B9A;&#x4E49;&#xFF0C;&#x5B9A;&#x4E49;&#x6293;&#x53D6;&#x7684;&#x6570;&#x636E;&#x7ED3;&#x6784;
&#x2502;   &#x251C;&#x2500;&#x2500; middlewares.py  # &#x5B9A;&#x4E49;Spider&#x548C;DownLoader&#x7684;Middlewares&#x4E2D;&#x95F4;&#x4EF6;&#x5B9E;&#x73B0;&#x3002; 
&#x2502;   &#x251C;&#x2500;&#x2500; pipelines.py    # &#x5B83;&#x5B9A;&#x4E49;Item Pipeline&#x7684;&#x5B9E;&#x73B0;&#xFF0C;&#x5373;&#x5B9A;&#x4E49;&#x6570;&#x636E;&#x7BA1;&#x9053;
&#x2502;   &#x251C;&#x2500;&#x2500; settings.py     # &#x5B83;&#x5B9A;&#x4E49;&#x9879;&#x76EE;&#x7684;&#x5168;&#x5C40;&#x914D;&#x7F6E;
&#x2502;   &#x2514;&#x2500;&#x2500; spiders         # &#x5176;&#x4E2D;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x4E2A;Spider&#x7684;&#x5B9E;&#x73B0;&#xFF0C;&#x6BCF;&#x4E2A;Spider&#x90FD;&#x6709;&#x4E00;&#x4E2A;&#x6587;&#x4EF6;
&#x2502;       &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;       &#x2514;&#x2500;&#x2500; __pycache__
&#x2514;&#x2500;&#x2500; scrapy.cfg    #Scrapy&#x90E8;&#x7F72;&#x65F6;&#x7684;&#x914D;&#x7F6E;&#x6587;&#x4EF6;&#xFF0C;&#x5B9A;&#x4E49;&#x4E86;&#x914D;&#x7F6E;&#x6587;&#x4EF6;&#x8DEF;&#x5F84;&#x3001;&#x90E8;&#x7F72;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x7B49;&#x5185;&#x5BB9;
</code></pre><h4 id="&#x2461;-&#x8FDB;&#x5165;demo&#x9879;&#x76EE;&#x76EE;&#x5F55;&#xFF0C;&#x521B;&#x5EFA;&#x722C;&#x866B;spider&#x7C7B;&#x6587;&#x4EF6;">&#x2461; &#x8FDB;&#x5165;demo&#x9879;&#x76EE;&#x76EE;&#x5F55;&#xFF0C;&#x521B;&#x5EFA;&#x722C;&#x866B;spider&#x7C7B;&#x6587;&#x4EF6;</h4>
<ul>
<li>&#x6267;&#x884C;genspider&#x547D;&#x4EE4;&#xFF0C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x662F;Spider&#x7684;&#x540D;&#x79F0;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x662F;&#x7F51;&#x7AD9;&#x57DF;&#x540D;&#x3002;</li>
</ul>
<pre><code>scrapy genspider fang fang.5i5j.com

$ tree 

&#x251C;&#x2500;&#x2500; demo
&#x2502;   &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;   &#x251C;&#x2500;&#x2500; __pycache__
&#x2502;   &#x2502;   &#x251C;&#x2500;&#x2500; __init__.cpython-36.pyc
&#x2502;   &#x2502;   &#x2514;&#x2500;&#x2500; settings.cpython-36.pyc
&#x2502;   &#x251C;&#x2500;&#x2500; items.py
&#x2502;   &#x251C;&#x2500;&#x2500; middlewares.py
&#x2502;   &#x251C;&#x2500;&#x2500; pipelines.py
&#x2502;   &#x251C;&#x2500;&#x2500; settings.py
&#x2502;   &#x2514;&#x2500;&#x2500; spiders
&#x2502;       &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;       &#x251C;&#x2500;&#x2500; __pycache__
&#x2502;       &#x2502;   &#x2514;&#x2500;&#x2500; __init__.cpython-36.pyc
&#x2502;       &#x2514;&#x2500;&#x2500; fang.py  #&#x5728;spiders&#x76EE;&#x5F55;&#x4E0B;&#x6709;&#x4E86;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x7C7B;&#x6587;&#x4EF6;fang.py
&#x2514;&#x2500;&#x2500; scrapy.cfg


# fang.py&#x7684;&#x6587;&#x4EF6;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;

# -*- coding: utf-8 -*-
import scrapy

class FangSpider(scrapy.Spider):
    name = &apos;fang&apos;
    allowed_domains = [&apos;fang.5i5j.com&apos;]
    start_urls = [&apos;http://fang.5i5j.com/&apos;]

    def parse(self, response):
        pass
</code></pre><ul>
<li><p>Spider&#x662F;&#x81EA;&#x5DF1;&#x5B9A;&#x4E49;&#x7684;&#x7C7B;&#xFF0C;Scrapy&#x7528;&#x5B83;&#x6765;&#x4ECE;&#x7F51;&#x9875;&#x4E2D;&#x6293;&#x53D6;&#x5185;&#x5BB9;&#xFF0C;&#x5E76;&#x89E3;&#x6790;&#x6293;&#x53D6;&#x7ED3;&#x679C;&#x3002;</p>
</li>
<li><p>&#x6B64;&#x7C7B;&#x7EE7;&#x627F;Scrapy&#x63D0;&#x4F9B;&#x7684;Spider&#x7C7B;scrapy.Spider&#xFF0C;&#x7C7B;&#x4E2D;&#x6709;&#x4E09;&#x4E2A;&#x5C5E;&#x6027;&#xFF1A;name&#x3001;allowed_domains&#x3001;start_urls&#x548C;&#x65B9;&#x6CD5;parse&#x3002;</p>
</li>
<li><p>name&#xFF1A;&#x662F;&#x6BCF;&#x4E2A;&#x9879;&#x76EE;&#x552F;&#x4E00;&#x540D;&#x5B57;&#xFF0C;&#x7528;&#x4E8E;&#x533A;&#x5206;&#x4E0D;&#x540C;Spider&#x3002;</p>
</li>
<li>allowed_domains: &#x5B83;&#x662F;&#x5141;&#x8BB8;&#x722C;&#x53D6;&#x7684;&#x57DF;&#x540D;&#xFF0C;&#x5982;&#x679C;&#x521D;&#x59CB;&#x6216;&#x540E;&#x7EED;&#x7684;&#x8BF7;&#x6C42;&#x94FE;&#x63A5;&#x4E0D;&#x662F;&#x8FD9;&#x4E2A;&#x57DF;&#x540D;&#xFF0C;&#x5219;&#x8BF7;&#x6C42;&#x94FE;&#x63A5;&#x4F1A;&#x88AB;&#x8FC7;&#x6EE4;&#x6389;</li>
<li>start_urls&#xFF1A; &#x5B83;&#x5305;&#x542B;&#x4E86;Spider&#x5728;&#x542F;&#x52A8;&#x65F6;&#x722C;&#x53D6;&#x7684;URL&#x5217;&#x8868;&#xFF0C;&#x521D;&#x59CB;&#x8BF7;&#x6C42;&#x662F;&#x7531;&#x5B83;&#x6765;&#x5B9A;&#x4E49;&#x7684;&#x3002;</li>
<li>parse&#x65B9;&#x6CD5;&#xFF1A; &#x8C03;&#x7528;start_urls&#x94FE;&#x63A5;&#x8BF7;&#x6C42;&#x4E0B;&#x8F7D;&#x6267;&#x884C;&#x540E;&#x5219;&#x8C03;&#x7528;parse&#x65B9;&#x6CD5;&#xFF0C;&#x5E76;&#x5C06;&#x7ED3;&#x679C;&#x4F20;&#x5165;&#x6B64;&#x65B9;&#x6CD5;&#x3002;</li>
</ul>
<h4 id="&#x2462;-&#x521B;&#x5EFA;item">&#x2462; &#x521B;&#x5EFA;Item</h4>
<ul>
<li><p>Item&#x662F;&#x4FDD;&#x5B58;&#x722C;&#x53D6;&#x6570;&#x636E;&#x7684;&#x5BB9;&#x5668;&#xFF0C;&#x5B83;&#x7684;&#x4F7F;&#x7528;&#x65B9;&#x6CD5;&#x548C;&#x5B57;&#x5178;&#x7C7B;&#x578B;&#xFF0C;&#x4F46;&#x76F8;&#x6BD4;&#x5B57;&#x5178;&#x591A;&#x4E86;&#x4E9B;&#x4FDD;&#x62A4;&#x673A;&#x5236;&#x3002;</p>
</li>
<li><p>&#x521B;&#x5EFA;Item&#x9700;&#x8981;&#x7EE7;&#x627F;scrapy.Item&#x7C7B;&#xFF0C;&#x5E76;&#x4E14;&#x5B9A;&#x4E49;&#x7C7B;&#x578B;&#x4E3A;scrapy.Field&#x7684;&#x5B57;&#x6BB5;&#xFF1A;&#xFF08;&#x6807;&#x9898;&#x3001;&#x5730;&#x5740;&#x3001;&#x5F00;&#x76D8;&#x65F6;&#x95F4;&#x3001;&#x6D4F;&#x89C8;&#x6B21;&#x6570;&#x3001;&#x5355;&#x4EF7;&#xFF09;</p>
</li>
<li><p>&#x5177;&#x4F53;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;</p>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> scrapy

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FangItem</span><span class="hljs-params">(scrapy.Item)</span>:</span>
    <span class="hljs-comment"># define the fields for your item here like:</span>
    title = scrapy.Field()
    address = scrapy.Field()
    time = scrapy.Field()
    clicks = scrapy.Field()
    price = scrapy.Field()
    <span class="hljs-comment">#pass</span>
</code></pre>
<h4 id="&#x2463;-&#x89E3;&#x6790;response">&#x2463; &#x89E3;&#x6790;Response</h4>
<ul>
<li><p>&#x5728;fang.py&#x6587;&#x4EF6;&#x4E2D;&#xFF0C;parse()&#x65B9;&#x6CD5;&#x7684;&#x53C2;&#x6570;response&#x662F;start_urls&#x91CC;&#x9762;&#x7684;&#x94FE;&#x63A5;&#x722C;&#x53D6;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
</li>
<li><p>&#x63D0;&#x53D6;&#x7684;&#x65B9;&#x5F0F;&#x53EF;&#x4EE5;&#x662F;CSS&#x9009;&#x62E9;&#x5668;&#x3001;XPath&#x9009;&#x62E9;&#x5668;&#x6216;&#x8005;&#x662F;re&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x3002;</p>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> scrapy
<span class="hljs-keyword">from</span> demo.items <span class="hljs-keyword">import</span> FangItem

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FangSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span>
    name = <span class="hljs-string">&apos;fang&apos;</span>
    allowed_domains = [<span class="hljs-string">&apos;fang.5i5j.com&apos;</span>]
    <span class="hljs-comment">#start_urls = [&apos;http://fang.5i5j.com/&apos;]</span>
    start_urls = [<span class="hljs-string">&apos;https://fang.5i5j.com/bj/loupan/&apos;</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span>
        hlist = response.css(<span class="hljs-string">&quot;div.houseList_list&quot;</span>)
        <span class="hljs-keyword">for</span> vo <span class="hljs-keyword">in</span> hlist:
            item = FangItem()
            item[<span class="hljs-string">&apos;title&apos;</span>] =  vo.css(<span class="hljs-string">&quot;h3.fontS20 a::text&quot;</span>).extract_first()
            item[<span class="hljs-string">&apos;address&apos;</span>] =  vo.css(<span class="hljs-string">&quot;span.addressName::text&quot;</span>).extract_first()
            item[<span class="hljs-string">&apos;time&apos;</span>] =  vo.re(<span class="hljs-string">&quot;&lt;span&gt;(.*?)&#x5F00;&#x76D8;&lt;/span&gt;&quot;</span>)[<span class="hljs-number">0</span>]
            item[<span class="hljs-string">&apos;clicks&apos;</span>] =  vo.re(<span class="hljs-string">&quot;&lt;span&gt;&lt;i&gt;([0-9]+)&lt;/i&gt;&#x6D4F;&#x89C8;&lt;/span&gt;&quot;</span>)[<span class="hljs-number">0</span>]
            item[<span class="hljs-string">&apos;price&apos;</span>] =  vo.css(<span class="hljs-string">&quot;i.fontS24::text&quot;</span>).extract_first()
            <span class="hljs-comment">#print(item)</span>
            <span class="hljs-keyword">yield</span> item
</code></pre>
<h4 id="&#x2464;&#x3001;&#x4F7F;&#x7528;item-pipeline">&#x2464;&#x3001;&#x4F7F;&#x7528;Item Pipeline</h4>
<ul>
<li>Item Pipeline&#x4E3A;&#x9879;&#x76EE;&#x7BA1;&#x9053;&#xFF0C;&#x5F53;Item&#x751F;&#x4EA7;&#x540E;&#xFF0C;&#x4ED6;&#x4F1A;&#x81EA;&#x52A8;&#x88AB;&#x9001;&#x5230;Item Pipeline&#x8FDB;&#x884C;&#x5904;&#x7406;&#xFF1A;</li>
<li>&#x6211;&#x4EEC;&#x5E38;&#x7528;Item Pipeline&#x6765;&#x505A;&#x5982;&#x4E0B;&#x64CD;&#x4F5C;&#xFF1A;<ul>
<li>&#x6E05;&#x7406;HTML&#x6570;&#x636E;</li>
<li>&#x9A8C;&#x8BC1;&#x6293;&#x53D6;&#x6570;&#x636E;&#xFF0C;&#x68C0;&#x67E5;&#x6293;&#x53D6;&#x5B57;&#x6BB5;</li>
<li>&#x67E5;&#x91CD;&#x5E76;&#x4E22;&#x5F03;&#x91CD;&#x590D;&#x5185;&#x5BB9;</li>
<li>&#x5C06;&#x722C;&#x53D6;&#x7ED3;&#x679C;&#x4FDD;&#x5B58;&#x5230;&#x6570;&#x636E;&#x5E93;&#x91CC;&#x3002;</li>
</ul>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DemoPipeline</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        print(item)
        <span class="hljs-keyword">return</span> item
</code></pre>
<ul>
<li>&#x8FDB;&#x5165;&#x914D;&#x7F6E;settings&#x4E2D;&#x5F00;&#x542F;Item Pipelines&#x7684;&#x4F7F;&#x7528;</li>
</ul>
<h4 id="&#x2465;&#x3001;&#x8FD0;&#x884C;&#xFF1A;">&#x2465;&#x3001;&#x8FD0;&#x884C;&#xFF1A;</h4>
<ul>
<li><p>&#x6267;&#x884C;&#x5982;&#x4E0B;&#x547D;&#x4EE4;&#x6765;&#x542F;&#x7528;&#x6570;&#x636E;&#x722C;&#x53D6;</p>
<pre><code>scrapy crawl fang
</code></pre></li>
<li><p>&#x5C06;&#x7ED3;&#x679C;&#x4FDD;&#x5B58;&#x5230;&#x6587;&#x4EF6;&#x4E2D;: &#x683C;&#x5F0F;&#xFF1A;json&#x3001;csv&#x3001;xml&#x3001;pickle&#x3001;marshal&#x7B49;</p>
</li>
</ul>
<pre><code>scrapy crawl fang -o fangs.json
scrapy crawl fang -o fangs.csv
scrapy crawl fang -o fangs.xml
scrapy crawl fang -o fangs.pickle
scrapy crawl fang -o fangs.marshal
</code></pre><h2 id="23-scrapy&#x6846;&#x67B6;&#x4E2D;&#x7684;post&#x63D0;&#x4EA4;&#xFF1A;">2.3 Scrapy&#x6846;&#x67B6;&#x4E2D;&#x7684;POST&#x63D0;&#x4EA4;&#xFF1A;</h2>
<ul>
<li>&#x5728;Scrapy&#x6846;&#x67B6;&#x4E2D;&#x9ED8;&#x8BA4;&#x90FD;&#x662F;GET&#x7684;&#x63D0;&#x4EA4;&#x65B9;&#x5F0F;&#xFF0C;&#x4F46;&#x662F;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>FormRequest</code>&#x6765;&#x5B8C;&#x6210;POST&#x63D0;&#x4EA4;&#xFF0C;&#x5E76;&#x53EF;&#x4EE5;&#x643A;&#x5E26;&#x53C2;&#x6570;&#x3002;</li>
<li><p>&#x5982;&#x4E0B;&#x6848;&#x4F8B;&#x4E3A;&#x6709;&#x9053;&#x8BCD;&#x5178;&#x7684;&#x7FFB;&#x8BD1;&#x4FE1;&#x606F;&#x722C;&#x53D6;&#x6848;&#x4F8B;&#xFF0C;&#x7F51;&#x5740;&#xFF1A;<code>http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule</code></p>
</li>
<li><p>&#x9996;&#x5148;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;<code>youdao</code>&#x6709;&#x9053;&#x7684;&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF1A;</p>
</li>
</ul>
<pre><code>scrapy genspider youdao fanyi.youdao.com
</code></pre><ul>
<li>&#x7F16;&#x5199;&#x722C;&#x866B;&#x6587;&#x4EF6;&#xFF0C;&#x6CE8;&#x610F;&#x8FD4;&#x56DE;&#x7684;&#x662F;json&#x683C;&#x5F0F;&#xFF0C;&#x5177;&#x4F53;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> scrapy,json


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">YoudaoSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span>
    name = <span class="hljs-string">&apos;youdao&apos;</span>
    allowed_domains = [<span class="hljs-string">&apos;fanyi.youdao.com&apos;</span>]
    <span class="hljs-comment">#start_urls = [&apos;http://fanyi.youdao.com&apos;]</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start_requests</span><span class="hljs-params">(self)</span>:</span>
        url = <span class="hljs-string">&apos;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&apos;</span>
        keyword = input(<span class="hljs-string">&quot;&#x8BF7;&#x8F93;&#x5165;&#x8981;&#x7FFB;&#x8BD1;&#x7684;&#x5355;&#x8BCD;&#xFF1A;&quot;</span>)
        data = {<span class="hljs-string">&apos;i&apos;</span>:keyword,<span class="hljs-string">&apos;doctype&apos;</span>: <span class="hljs-string">&apos;json&apos;</span>,}
        <span class="hljs-comment"># FormRequest &#x662F;Scrapy&#x53D1;&#x9001;POST&#x8BF7;&#x6C42;&#x7684;&#x65B9;&#x6CD5;</span>
        <span class="hljs-keyword">yield</span> scrapy.FormRequest(
            url = url,
            formdata = data,
            callback = self.parse
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span>
        res = json.loads(response.body)
        print(res[<span class="hljs-string">&apos;translateResult&apos;</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&apos;tgt&apos;</span>])
        input(<span class="hljs-string">&quot;&#x6309;&#x4EFB;&#x610F;&#x952E;&#x7EE7;&#x7EED;&quot;</span>)
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="1.html" class="navigation navigation-prev " aria-label="Previous page: 1. Scrapy框架介绍与安装">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="3.html" class="navigation navigation-next " aria-label="Next page: 3. Selector选择器">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"2. Scrapy框架的使用","level":"1.3.2","depth":2,"next":{"title":"3. Selector选择器","level":"1.3.3","depth":2,"path":"week01/3.md","ref":"week01/3.md","articles":[]},"previous":{"title":"1. Scrapy框架介绍与安装","level":"1.3.1","depth":2,"path":"week01/1.md","ref":"week01/1.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"week01/2.md","mtime":"2018-05-22T02:14:19.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2018-06-06T01:59:17.229Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

